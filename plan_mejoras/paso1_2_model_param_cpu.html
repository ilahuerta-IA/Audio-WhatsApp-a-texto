<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Detalle Paso 1.2: Parametrizar Modelo Whisper (CPU)</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <h3>Sub-Detalle Paso 1.2: Parametrizar Selección de Modelo Whisper (CPU)</h3>
        <p><strong>Objetivo Específico:</strong> Permitir la selección y uso de diferentes modelos de Whisper que sean razonablemente ejecutables en CPU (ej: 'tiny', 'base', 'small'), configurando cuál usar.</p>
        <p><strong>Estado:</strong> <span class="status status-pendiente">Pendiente</span></p>

        <h4>Proceso Conceptual Detallado:</h4>
        <ol class="detalle-pasos">
            <li>
                <strong>1.2.1: Modificar `config.py`</strong>
                - Añadir una constante para la lista de modelos recomendados para CPU, por ejemplo:
                  `WHISPER_CPU_MODELS = ['tiny', 'base', 'small']`
                - Modificar (o confirmar) la constante `WHISPER_MODEL_NAME` para que actúe como el modelo *por defecto* a usar si no se especifica otro. Asegurarse de que sea uno de la lista anterior (ej: `'tiny'` o `'base'`).
                  `WHISPER_MODEL_NAME = 'tiny' # Modelo por defecto`
                <p class="nota">Esto centraliza la configuración inicial.</p>
            </li>
            <li>
                <strong>1.2.2: Modificar Carga del Modelo en `whisper_transcriber.py`</strong>
                - Asegurar que la función `_load_model_global` (o donde se llame a `whisper.load_model`) use el valor de `config.WHISPER_MODEL_NAME` dinámicamente, en lugar de tener 'tiny' escrito directamente en la llamada.
                - La llamada dentro de `_load_model_global` debería ser algo como:
                  `loaded_model = whisper.load_model(model_name_to_load, device='cpu')`
                  donde `model_name_to_load` es un parámetro de la función (inicialmente podría ser `config.WHISPER_MODEL_NAME`).
                - Asegurar que el `device` se fije a `'cpu'`, ya que la detección del paso 1.1 ya no se usará para elegir 'cuda' en esta estrategia. *(Podríamos mantener la lógica de `device_to_use` por si en el futuro se añade GPU local, pero por ahora la carga siempre usará CPU)*.
                <p class="nota">Simplificación: Forzar `device='cpu'` en la carga, ignorando temporalmente `self.device_to_use` de la GUI, ya que no habrá opción 'cuda' viable.</p>
            </li>
            <li>
                <strong>1.2.3: (Opcional) Permitir Selección en UI</strong>
                - Si se desea flexibilidad sin editar `config.py`, añadir un widget (ej: `ttk.Combobox`) en `gui.py` para que el usuario seleccione uno de los modelos de `config.WHISPER_CPU_MODELS` antes de transcribir.
                - El valor seleccionado en la UI debería sobrescribir el `config.WHISPER_MODEL_NAME` por defecto al iniciar la carga del modelo.
                - Esto requiere pasar el modelo seleccionado desde la GUI (`AudioTranscriptorPro`) a la clase `WhisperTranscriber` y/o a la función de carga.
            </li>
            <li>
                <strong>1.2.4: Pruebas</strong>
                - Cambiar el valor de `config.WHISPER_MODEL_NAME` a `'base'` y ejecutar. Verificar que el modelo 'base' se carga y se usa.
                - Cambiar a `'small'` y probar. Observar el tiempo de carga y de transcripción (serán mayores).
                - Confirmar que la aplicación sigue funcionando correctamente con los diferentes modelos (CPU).
            </li>
        </ol>
        <p><strong>Conclusión del Paso 1.2:</strong> La aplicación permitirá configurar y usar diferentes modelos de Whisper (enfocados en CPU), ofreciendo un compromiso ajustable entre velocidad y precisión según las capacidades de la máquina del usuario.</p>

        <a href="bloque1_whisper.html" class="nav-back">← Volver al Bloque 1</a>
    </div>
</body>
</html>